---
title: "Exploring Zstandard user-provided dictionary compression for FASTA files"
format:
  plos-pdf:
    number-sections: false
    journal:
      # This is an identifier for the target journal: 
      # from https://plos.org/resources/writing-center/ following submission guidelines link, the identifier is the part of the URL after https://journals.plos.org/<id>/s/submission-guidelines
      id: plosbiology
    include-in-header: |
      % Remove comment for double spacing
      % \usepackage{setspace} 
      % \doublespacing
author:
  - name: Michael Persico
    equal-contributor: false
    affiliations:
      - ref: aff1
    notes: |
      "Current Address: Dept/Program/Center, Institution Name, City, State, Country"
    corresponding: false
    email: correspondingauthor@institute.edu
author-notes:
  equal-contributor: These authors contributed equally to this work.
  deceased: Deceased
  group: Membership list can be found in the Acknowledgments sections
affiliations:
  - id: aff1
    name: Department of Biology, Concordia University
    city: Montreal
    state: Quebec
    country: Canada
abstract: |
  ## Background

  Zstandard (Zstd) represents a universal, lossless data compression standard and implementation that is highly configurable and is aimed at coupling high compression ratios with fast compression/decompression performance. Previous studies have paired specific Zstd configurations with various file formats in bioinformatics to reduce total data volume. This paper presents a primitive "training mode" model, written in the Julia programming language, wherein a custom compression dictionary is generated from a sample FASTA set in order to explore further compression improvements and compare them to the compression performance of Xz, Zlib, Bzip2, and Lz4 universal compressors.
  
  ## Results

  ## Conclusions
bibliography: bibliography.bib 
---

# Introduction

The explosion of biological data has represented a significant topic of research, with a number of challenges presented over subsequent generations of technological development in regards to the management of  the increasing volume and complexity of data[@d2018high;@li2014big]. In response, emerging trends in data management have lead to the development of novel, scalable methods for the efficient transmission and storage of large amounts of data[@sais2022intelligent]. With a potentially exponential quantity of files, datasets, and other data resources to be handled, data compression represents a method for reducing overall resource size by encoding the original data into a compact form, thus helping to ease storage requirements [@jayasankar2021survey]. Research into data compression in the context of biological data began to pick up near the turn of the 21st century as universal compression algorithms at the time were not considered ideal for compressing DNA or RNA sequence data well, which led to the introduction of purpose-built algorithms that addressed the unique peculiarities of genomic data[@grumbach1994new]. At the same time, new file formats were introduced, either text-based or binary-based, for more accurate structuring and representation of biological data, complementing new software tools[@lipman1985rapid;@mills2014common].

The FASTA file format is a legacy of the original FASTA program for finding sequence similarities with a query sequence[@lipman1985rapid]. Each file can possess multiple sequences, each paired with a description line distinguished by a ">" symbol followed by arbitrary text, usually a name and/or summary description, on the same line. It is a commonly supported file format in bioinformatics and has been the target for optimized data compressors with competing claims for performance. The DELIMINATE lossless algorithm was first proposed in 2012, wherein header and sequence data are separated into DELIM-1 and DELIM-2 variants and a two-phase process is pursued involving delta encoding, progressive elimination of nucleotide characters, and 7-Zip archiving[@mohammed2012deliminate]. The claims of better compression/decompression performance of FASTA files were soon rivaled by the introduction of the MFCompress tool, again separating headers and sequence data but instead relying on probabilistic models to encode the data[@pinho2014mfcompress], which was then countered by the Nucleotide Archival Format, a novel file format noteworthy in this context for the inclusion of a Zstandard compression step[@kryukov2019nucleotide].

IETF RFC 8878, introduced by engineers at Facebook, defines Zstandard as a lossless data compression/decompression format[@collet2021rfc8478]. It is often abbreviated as "Zstd", though such can also refer to Facebook's own implementation of the algorithm written mostly in C[@facebook]. Content is sliced and packaged into "frames" that are independent of one another defined as either compressed data Zstandard frames or Skippable frames containing custom user metadata[@collet2021rfc8478]. Zstd's backbone is the use of Finite State Entropy and Huffman entropy encoding schemes that replace data with coded forms independently of the medium[@ezhilarasan_thambidurai_praveena_srinivasan_sumathi_2007;@LU20171], with the former compressing all symbols, though header information is first encoded by the latter[@collet2021rfc8478]. Zstd, for small data compression improvements, also functions as a dictionary coder, meaning that although the algorithm is universal in the sense of being applicable to a number of both text-based and binary-based data files, it can be optimized for compacting characteristic data by "training" Zstd with a collection of sample files to build a set of common patterns that allow for substitutions when compressing/decompressing, allowing for further  gains for similar data[@8316293]. 

Common bioinformatics file formats often include a set structure with the express purpose of representing specific kinds of biological data composed of repeating elements, as is the case with FASTA files with either nucleotide or amino acid sequences. In this work is described a pipeline for building Zstd dictionaries via FASTA datasets along with a comparison of Zstd compression/decompression performance with that of several alternative lossless compressors for select datasets.

# Materials and methods 

All resources for the paper are included in the public Github repository at github.com/M-PERSIC/Persico2022.git.

The model contains two pipelines, one for Zstd custom dictionary generation and another for generating transcoded compression data using Bzip2, Xz, zLib, and Zstd compressors, with a general overview in @fig-model. Julia was chosen as there is package support for working with sequence data as well as for trivial transcoding of compressed or decompressed data via TranscodingStreams.jl wherein compression algorithm implementations are loaded as codecs[@sato]. A list of all direct dependencies can be found in the repository's Project.toml file. The Julia implementation operates by first unloading a tarball containing the set of FASTA training files into a Julia artifact, or life-cycled datastore. The files are plugged into Zstd's Dictionary API to generate a custom dictionary specific to that dataset. Random FASTA data is generated using BioSequences.jl[@biosequences] and FASTX.jl[@fastx] APIs which is then transcoded into compressed formats via the CodecBzip2.jl[@codecbzip2], CodecXz.jl[@codecxz], CodecZlib.jl[@codeczlib], and CodecZstd.jl[@codeczstd] codecs for comparative analysis. Both dictionary-based and default Zstd compression are applied to determine if the former desmontrates a significant reduction in data size.

![Overview of the Julia implementation for the compression model. **A**, Custom dictionary generation via an Julia artifact of FASTA files plugged into the Zstd Dictionary API. **B**, Transcoding randomly generated FASTA formatted data using TranscodingStreams.jl[@sato] compression codecs. ](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/figures/Compression_Pipeline_Model.png){#fig-model}

# Results

# Discussion

The provided FASTA training dataset was retrieved from a set of \textit{E.coli} MS 200-1 strain genomic scaffolds from a whole genome shotgun run[@ncbi]. Sampling a single organism may allow for additional compression/decompression improvements as the sequence data could possess repeating patterns unique to that individual's or species' genome that are recognized and added to the Zstd dictionary during training. The FASTA files themselves may bear similarity with one another in the same set, such as each file being of similar length. Simultaneously, the Zstd dictionary may not achieve comparable compression/decompression performance when attempted on more dissimilar sequence data. Portability of Zstd dictionaries, in the sense of being universally applicable across all sequence data, could be weighed against generating dedicated dictionaries for specific datasets depending on the storage requirements.  Zstd parameters were kept default, though they can be manually altered, including advanced compression options like compression job size or selecting from 22 predefined compression levels[@facebook].


# Supporting information 

::: {.supp}
## S1 Fig. {#s1-fig}

Bold the title sentence.

Add descriptive text after the title of the item (optional).
:::

::: {.supp}
## S2 Fig. {#s2-fig}

Lorem ipsum.

Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.
:::


::: {.supp}
## S1 File. {#s1-file}

Lorem ipsum.

:::

::: {.supp}

## S1 Video. {#s1-video}

Lorem ipsum.

Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.
:::

::: {.supp}
## S1 Appendix. {#s1-appendix}

Lorem ipsum.

Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.
:::

::: {.supp}
## S1 Table. {#s1-table}

Lorem ipsum.

Maecenas convallis mauris sit amet sem ultrices gravida. Etiam eget sapien nibh. Sed ac ipsum eget enim egestas ullamcorper nec euismod ligula. Curabitur fringilla pulvinar lectus consectetur pellentesque.
:::

# Acknowledgments

I would like to express my sincere gratitude to Professor David Walsh for his advice that helped shape the research as it progressed; My friends and family that supported me throughout my studies; The Julia community for their support before and throughout the writing of this paper; The Quarto developers for producing the Quarto publishing system[@Allaire_Quarto_2022] along with the PLOS template used for this paper; Luca Di Maio and contributors to the Distrobox tool for the ease of setting up the containerized development environments[@maio]; Maximiliano Sandoval and contributors to the Citations app for managing the paper's bibliography[@sandoval], and all other persons that had indirectly assisted through their programs and research.
