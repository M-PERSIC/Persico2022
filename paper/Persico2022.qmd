---
title: "Exploring Zstandard user-provided dictionary compression for FASTA files"
format:
  plos-pdf:
    number-sections: false
    journal:
      # This is an identifier for the target journal: 
      # from https://plos.org/resources/writing-center/ following submission guidelines link, the identifier is the part of the URL after https://journals.plos.org/<id>/s/submission-guidelines
      id: plosbiology
    include-in-header: |
      % Remove comment for double spacing
      % \usepackage{setspace} 
      % \doublespacing
author:
  - name: Michael A. Persico
    equal-contributor: false
    affiliations:
      - ref: aff1
    notes: |
      "Current Address: Dept/Program/Center, Institution Name, City, State, Country"
    corresponding: false
    email: correspondingauthor@institute.edu
author-notes:
  equal-contributor: These authors contributed equally to this work.
  deceased: Deceased
  group: Membership list can be found in the Acknowledgments sections
affiliations:
  - id: aff1
    name: Department of Biology, Concordia University
    city: Montreal
    state: Quebec
    country: Canada
abstract: |
  Zstandard (Zstd) represents a universal, lossless data compression standard and implementation that is highly configurable and is aimed at coupling high compression ratios with fast compression/decompression performance. Previous studies have paired specific Zstd configurations with various file formats in bioinformatics to reduce total data volume. This paper presents a primitive "training mode" model, written in the Julia programming language, wherein a custom compression dictionary is generated from a sample FASTA set in order to explore further compression improvements and compare them to the compression performance of Xz, Zlib, Bzip2, and Lz4 universal compressors. Zstd dictionary-based compression did not yield significant compression gains (P < 0.05 in many compressor contrast scenarios using Tukey's HSD(honestly significant difference) test) and had performed worse than alternative compressor types. As an *E.coli* training dataset was used to generate a dictionary to compress randomized data, and with the abundance of options available for manually tuning Zstd's parameters, further compression improvements may be possible.

bibliography: bibliography.bib 
---

# Introduction

The explosion of biological data has represented a significant topic of research, with a number of challenges presented over subsequent generations of technological development in regards to the management of  the increasing volume and complexity of data[@d2018high;@li2014big]. In response, emerging trends in data management have lead to the development of novel, scalable methods for the efficient transmission and storage of large amounts of data[@sais2022intelligent]. With a potentially exponential quantity of files, datasets, and other data resources to be handled, data compression represents a method for reducing overall resource size by encoding the original data into a compact form, thus helping to ease storage requirements [@jayasankar2021survey]. Research into data compression in the context of biological data began to pick up near the turn of the 21st century as universal compression algorithms at the time were not considered ideal for compressing DNA or RNA sequence data well, which led to the introduction of purpose-built algorithms that addressed the unique peculiarities of genomic data[@grumbach1994new]. At the same time, new file formats were introduced, either text-based or binary-based, for more accurate structuring and representation of biological data, complementing new software tools[@lipman1985rapid;@mills2014common].

The FASTA file format is a legacy of the original FASTA program for finding sequence similarities with a query sequence[@lipman1985rapid]. Each file can possess multiple sequences, each paired with a description line distinguished by a ">" symbol followed by arbitrary text, usually a name and/or summary description, on the same line. It is a commonly supported file format in bioinformatics and has been the target for optimized data compressors with competing claims for performance. The DELIMINATE lossless algorithm was first proposed in 2012, wherein header and sequence data are separated into DELIM-1 and DELIM-2 variants and a two-phase process is pursued involving delta encoding, progressive elimination of nucleotide characters, and 7-Zip archiving[@mohammed2012deliminate]. The claims of better compression/decompression performance of FASTA files were soon rivaled by the introduction of the MFCompress tool, again separating headers and sequence data but instead relying on probabilistic models to encode the data[@pinho2014mfcompress], which was then countered by the Nucleotide Archival Format, a novel file format noteworthy in this context for the inclusion of a Zstandard compression step[@kryukov2019nucleotide].

IETF RFC 8878, introduced by engineers at Facebook, defines Zstandard as a lossless data compression/decompression format[@collet2021rfc8478]. It is often abbreviated as "Zstd", though such can also refer to Facebook's own implementation of the algorithm written mostly in C[@facebook]. Content is sliced and packaged into "frames" that are independent of one another defined as either compressed data Zstandard frames or Skippable frames containing custom user metadata[@collet2021rfc8478]. Zstd's backbone is the use of Finite State Entropy and Huffman entropy encoding schemes that replace data with coded forms independently of the medium[@ezhilarasan_thambidurai_praveena_srinivasan_sumathi_2007;@LU20171], with the former compressing all symbols, though header information is first encoded by the latter[@collet2021rfc8478]. Zstd, for small data compression improvements, also functions as a dictionary coder, meaning that although the algorithm is universal in the sense of being applicable to a number of both text-based and binary-based data files, it can be optimized for compacting characteristic data by "training" Zstd with a collection of sample files to build a set of common patterns that allow for substitutions when compressing/decompressing, allowing for further  gains for similar data[@8316293]. 

Common bioinformatics file formats often include a set structure with the express purpose of representing specific kinds of biological data composed of repeating elements, as is the case with FASTA files with either nucleotide or amino acid sequences. In this work is described a pipeline for building Zstd dictionaries via FASTA datasets along with a comparison of Zstd compression/decompression performance with that of several alternative lossless compressors for select datasets.

# Materials and methods 

All resources for the paper are included in the public Github repository at [github.com/M-PERSIC/Persico2022.git](https://www.github.com/M-PERSIC/Persico2022.git).

The model contains two pipelines, one for Zstd custom dictionary generation and another for generating transcoded compression data using Bzip2, Xz, zLib, and Zstd compressors, with a general overview in @fig-model. Julia was chosen as there is package support for working with sequence data as well as for trivial transcoding of compressed or decompressed data via TranscodingStreams.jl wherein compression algorithm implementations are loaded as codecs[@sato]. A list of all direct dependencies can be found in the repository's Project.toml file. The Julia implementation operates by first unloading a tarball containing the set of FASTA training files into a Julia artifact, or life-cycled datastore. The files are plugged into Zstd's training mode, via the Zstd command line interface --train option, to generate a custom dictionary specific to that dataset that is then plugged again into the CLI for data compression. Random FASTA data is generated using BioSequences.jl[@biosequences] and FASTX.jl[@fastx] APIs which is then transcoded into compressed formats via the CodecBzip2.jl[@codecbzip2], CodecXz.jl[@codecxz], CodecZlib.jl[@codeczlib], and CodecZstd.jl[@codeczstd] codecs for comparative analysis. Both dictionary-based and default Zstd compression are applied to determine if the former results in a significant reduction of overall data size.

![Overview of the Julia implementation for the compression model. **A**, Custom dictionary generation via an Julia artifact of FASTA files plugged into the Zstd Dictionary API. **B**, Transcoding randomly generated FASTA formatted data using TranscodingStreams.jl[@sato] compression codecs.](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/diagrams/Compression_Model_Overview.png){#fig-model}

\newpage 

# Results

![Average FASTA formatted data size, in bytes, following Bzip2, Xz, Zlib, default Zstd and dictionary-based Zstd compression. *Escherichia coli* MS 200-1 (NZ_ADUC00000000) artifact for Zstd dictionary generation[23]. Randomized DNA sequences within the range between 100 000 to 150 000 base pairs.](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/plots/Compression_Performance_Boxplot.png){#fig-performance}

![Tukey's HSD test for determining statistical significance between the compression performance, based on compressed data size, of Bzip2, Xz, Zlib, Zstd with default parameters, and Zstd with a custom dictionary based on the E.coli training dataset[23].](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/plots/Tukeytable.png){#fig-tukey}

\newpage

The Tukey's HSD test paints a picture of the statistical significance of the means of the compressed set of FASTA formatted data between each compressor type. As expected, uncompressed data size and compressed data size differ significantly, with no compressed FASTA file above more than 60 kilobytes based on @fig-performance. With dictionary-based compression applied, Zstd performs similarly to Zstd compression with default parameters (P > 0.05) and, based on the trends of the boxplot and the Tukey's test results, in almost every instance aside from default Zstd compression, dictionary-based compression performed worse than the competing compressor types.

# Discussion

As stated in the Results section, Zstd dictionary-based compression results were surprising in the sense that, at least with the configuration (particular *E.coli* dataset, specific number of randomized FASTA files, completely randomized DNA, RNA, or amino acid sequences), it performed either on par, as was the case with default Zstd compression, or worse than the alternative compressors. The provided FASTA training dataset was retrieved from a set of \textit{E.coli} MS 200-1 strain genomic scaffolds from a whole genome shotgun run[@ncbi]. Sampling a single organism may allow for additional compression/decompression improvements as the sequence data could possess repeating patterns unique to that individual's or species' genome that are recognized and added to the Zstd dictionary during training. The FASTA files themselves may bear similarity with one another in the same set, such as each file being of similar length. Simultaneously, the Zstd dictionary may not achieve comparable compression/decompression performance when attempted on more dissimilar sequence data, which might explain the observed results as randomized sequences were generated for compression performance analysis. Portability of Zstd dictionaries, in the sense of being universally applicable across all sequence data, could be weighed against generating dedicated dictionaries for specific datasets depending on the storage requirements. All other Zstd parameters were kept default, though they can be manually altered, including advanced compression options like compression job size or selecting from 22 predefined compression levels[@facebook].

Zstd's multifarious design warrants further research in how tuning the implementation by altering available options (block size, compression level,...) might yield more significant compression performance. The development of novel data toolsets have utilized Zstd in unique ways. Aside from the Nucleotide Archival Format, FASTAFS was recently introduced as a filesystem in userspace (FUSE) toolkit wherein FASTA files are pooled into a virtualization abstraction layer that archives the set of data synced with its metadata, generating a high-level interface for accessing and storing FASTA data[@fastafs]. The zstd-seekable compression library is an embedded component that keeps the FASTAFS archive compressed for storage reduction purposes, and it represents an alternative compressed data format that deviates from Zstandard, albeit in a compatible manner, that allows for partial decompression of subranges of data[@facebook]. This is achieved via independently compressed/decompressed Zstd frames, which FASTAFS exploits for fast random access to FASTA files by decompressing only data of interest. Further areas of research for general lossless data compression in the context of biological data could include exploring other properties like compression/decompression speed performance, meaning how fast data transcoding occurs, or exploring proof-of-concept algorithms that attempt novel strategies.

\newpage

# Supplementary Material 

![Average FASTA formatted data size, in bytes, following Bzip2, Xz, Zlib, default Zstd and dictionary-based Zstd compression. *Escherichia coli* MS 200-1 (NZ_ADUC00000000) artifact for Zstd dictionary generation. In this instance, randomized DNA sequences were generated within a shorter range between 100 000 to 150 000 base pairs. The discrepancy between uncompressed and compressed data is less pronounced than in @fig-performance, though no compressed FASTA file exceeds more than four kilobytes in size.](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/plots/SupplementaryPlotOne.png){#fig-rna}

![Average FASTA formatted data size, in bytes, following Bzip2, Xz, Zlib, default Zstd and dictionary-based Zstd compression. *Escherichia coli* MS 200-1 (NZ_ADUC00000000) artifact for Zstd dictionary generation. In this instance, randomized RNA sequences were generated within the range between 100 000 to 150 000 base pairs. The results visually remain similar to that of @fig-performance, although the compression performance of dictionary-based Zstd compression appears to have slightly regressed, perhaps due to some improvements in data compression gained via dictionary use no longer being possible due the difference in sequence type (substitution of thymine with uracil).](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/plots/SupplementaryPlotTwo.png){#fig-rna}

![Average FASTA formatted data size, in bytes, following Bzip2, Xz, Zlib, default Zstd and dictionary-based Zstd compression. *Escherichia coli* MS 200-1 (NZ_ADUC00000000) artifact for Zstd dictionary generation. In this instance, randomized amino acid sequences were generated within the range between 100 000 to 150 000 codons. Zstd default and dictionary-based compression performance remain similar, it being assumed that the more complex sequence type (20 possible codons versus four possible nucleotides per position) may have erased dictionary compression improvements trained from a DNA sequence dataset.](/var/home/mpersico/distrobox/ubuntu-zstd/Persico2022/assets/plots/SupplementaryPlotThree.png){#fig-rna}

\newpage

# Acknowledgments

I would like to express my sincere gratitude to Professor David Walsh for aiding me throughout the project and for teaching the bioinformatics course; My friends and family that supported me throughout my studies; The Julia community for their support before and throughout the writing of this paper; Mark Kittisopikul for developing the CodecZstd.jl #dictionary_and_parameters branch for Zstd dictionary functions; The Quarto developers for producing the Quarto publishing system[@Allaire_Quarto_2022] along with the PLOS template used for this paper; Luca Di Maio and contributors to the Distrobox tool for the ease of setting up the containerized development environments[@maio]; Maximiliano Sandoval and contributors to the Citations app for managing the paper's bibliography[@sandoval], and all other persons that had indirectly assisted through their programs and research.

# Conflict of interest

The author declares no conflict of interest.